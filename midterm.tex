\documentclass{article}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{array}
\usepackage{amsmath}
\usepackage{graphicx}
\author{Ed Jee} 
\title{Midterm: Dippel 2014}


\begin{document}
\maketitle

\section{Replication}
My strategy is as follows:

\begin{itemize}
    \item Create controls/instruments etc.
    \item Create model formulae programmatically.
    \item Estimate models using the package `fixest'.
    \item Compare replicated and original estimates programmatically using
    R's `testthat' library, originally designed for unit tests.  
\end{itemize}

The controls are naturally grouped by the author as follows:

\begin{table}[htbp]
    \centering\begin{tabular}{| m{4cm} | m{5cm} | m{5cm} | }
        \hline 
    Set & Controls  & Variable Name\\ \hline\hline
    Reservation Controls & log(local per capita  income),
    log(local unemployment),
    log(distance to nearest city),
    log(ruggedness),
    log(reservation area squared) &
    \textit{
        logpcinc\_co,
        logunempl\_co,
        logdist,
        logruggedness,
        logresarea\_sqkm
    }  \\  \hline
    Tribe Controls & subsistence patterns,
    sedentariness,
    wealth distinctions,
    social complexity &
    \textit{
        ea\_v5,
        ea\_v30,
        ea\_v32,
        ea\_v66
    }\\ \hline
    Extra Reservation Controls & log(population),
    log(population)$^2$,
    adult population share,
    casino present &
    \textit{
        logpop,
        logpopsq,
        popadultshare,
        casion
    }\\ \hline
    IV controls & ancestral ruggedness,
    distance from ancestral lands,
    local historical mining value (gold \& silver  ) 
    & \textit{
        homelandruggedness,
        removal,
        wgold\_enviro,
        wsilver\_enviro
    }\\ 
    \hline
    
    \end{tabular}
    \caption{Control List}
    \label{controls}
\end{table}

\subsection{ OLS and Fixed Effects}
First, I replicate the OLS and FE tables using the estimating equations:

\begin{align*}
    \log(p.c. \ income)_i &= \alpha + \beta_1 \text{Forced coexistence}_i   + \beta_2 \text{Historical centralisation}_i + X_i \Gamma  + \varepsilon_i
\end{align*}
 where $X_i$ corresponds to a matrix of covariates from Table \ref{controls}. The 
 fixed effects model is identical except we control for tribe fixed effects, given by \textit{eaid} in the dataset. State fixed 
 effects correspond to controlling for \textit{statenumber} which is not a 1:1 mapping with traditional states but a measure 
 constructed by the author, ostensibly to overcome issues of ancestral homelands overlapping multiple modern states. Standard 
 errors are clustered at the tribe and state level (\textit{eaid, statenumber}). Standard errors are clustered the conventional way using 
 Eicker-Huber-White with any clustered degrees of freedom adjustments using Cameron-Gelbach-Miller. I omit the OLS estimates here for brevity
 and only display the FE estimates since they offer a more plausible selection on observables argument. 
% \input{data/output/table-2-ols.tex}




% \input{data/output/table-4-A.tex}
% \input{data/output/table-4-B.tex}



\input{data/output/table-2-fe.tex}
Both OLS and FE replicated estimates are identical to Dippel (2015) estimates 
after rounding to three decimal places. $t$-statistics are similarly identical apart 
from column 5 - this seems likely to be due to differences in counting degrees of freedom in clusters
with few/singleton observations when using two way fixed effects as it only occurs once we control for state and tribe fixed effects. 
Regardless, the estimates are still significant and there's no material difference in estimates.

\subsection{Instrumental Variables}
I estimate the following:

\begin{align*}
    \log(p.c. \ income)_i &= \alpha + \beta_1 \widehat{\text{Forced coexistence}}_i   + \beta_2 \text{Historical centralisation}_i + X_i \Gamma  + \varepsilon_i \\ \\
\text{Forced coexistence}_i &= \pi_0 + \pi_1 \text{Historical gold-mining}_i + \pi_2 \text{Historical silver-mining} + X_i \Pi_3 + u_i
\end{align*}

Where historical gold and silver mining are our instruments for the endogenous variable "Forced coexistence". The "single instrument" specification preferred by the author instead sums across gold and silver mining to produce "Historical mining".
Standard errors are clustered as described in the OLS and FE cases above.


IV estimates replicate less cleanly: columns 1, 2, and 3 replicate perfectly; columns 4, 5, and 6 can be a little different, although at first glance not sizably different. The author 
adds sets of controls in identical stages as in the OLS and FE estimation strategies and the number of observations is identical. Since columns 1-3 
replicate perfectly and column 4 fails it seems likely there's a discrepancy between the additional reservation controls the author defines during OLS
and IV. I omit Panel A in the interest of space as Dippel describes the single instrument specification as his preferred. Comparing the final column in the preferred specification 
we see a difference of $-0.486 - - 0.443 = -0.043 \approx 10\%$ difference.

% \input{data/output/table-5-iv-A.tex}
\input{data/output/table-5-iv-B.tex}

 The discrepancy in columns 4,5, and 6 continue in the first stage and reduced form breakdowns in Table 3 (omitted here for brevity). This is
somewhat worrying since this leads to the gold mining instrument becoming insignificant in specifications 4 and 6 and a further reduction in an already small 
first stage $F$-statistic. 

\section{Extensions}
\subsection*{Anderson-Rubin}

First, I estimate Anderson-Rubin weak IV robust confidence intervals since the first stage $F$-stats reported in the author's preferred 
specification are still somewhat low with the majority of columns 1-5 in Table 3 (Dippel Table 5) barely above the common heuristic of 10. Even
without conditioning on $F$-stat pre-tests computing weak IV robust confidence intervals is sensible.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{data/output/anderson-rubin-plot.png}
    \caption{Anderson Rubin vs Original Confidence Intervals}
    \label{AR}
\end{figure}
In Figure \ref{AR} I plot AR confidence intervals alongside the author's conventional clustered standard error confidence intervals using the Dippel Table 5, Panel B specification (IV - one instrument).
Clearly, this leads to a large increase in the CI width and all but the final estimate are now insignificant from 0. We should doubt somewhat the author's primary conclusion as the results
do not appear to be robust to weak instruments.
\subsection*{Counterfactual Densities}
In order to explore potential outcome densities using the results of \cite{imbens-rubin}
I discretise the instrument and treatment to create binary variables where each variable is set to $1$ if the realisation is above its
state median - using means instead of medians leads to only $\approx 3$ compliers due to the fat tailed nature of the data, as opposed to $\approx 35$ using medians.


\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{data/output/density-plot.png}
    \caption{Density Plots}
    \label{dens}
\end{figure}


The density for $Y_1$ is clearly shifted to the left of the potential outcome density $Y_0$ which makes sense given the negative effect 
of forced coexistence estimated by the author\footnote{Although given he uses the instrument in its continuous, not binary form, these LATEs aren't directly comparable}. Interestingly, the $Y_1$ density is almost uniformly to the left of the $Y_0$ density i.e. 
every quantile of the distribution performs worse under treatment although we cannot make any statements about individuals without 
further assumptions such as rank preservation. The estimated densities for always- and never-takers are somewhat different to the compliers 
and it seems likely we can't extrapolate the LATE estimate to other sub-populations.
\subsection*{Additional Robustness Checks}

Since the paper's main finding doesn't seem to be robust to weak instruments I explore additional robustness checks using 
\cite{amip}'s Automatic Finite-Sample Robustness Metric. This identifies which in-sample data points have maximal influence on 
 metrics of interest such as the significance or sign of a result. Whilst perfectly identifying these points is computationally infeasible
since it involves iterating through the power set of all data points and deleting $k = 1, 2, 3, ...$ data points the author's calculate approximate 
maximal influence in one step using a Taylor expansion.



\input{data/output/amip.tex}

The robustness checks again suggest we should be cautious interpretting the author's primary result - deleting only 7 data points leads to insignificance and 
removing 19 completely reverses the sign of the estimate.

\subsection*{Spatial Noise}
Finally, I investigate whether the author's estimates could merely be the result of fitting spatial noise. \cite{persistence} shows that many modern 
papers in economics exhibit unusually large $t$-statistics due to regressing modern outcomes on historical characteristics without adequately accounting for 
spatial noise. 

Kelly suggests two solutions\footnote{He notes that commonly used Conley standard errors often use a bandwidth magnitudes too small to account for reasonable levels of spatial correlation.}: Reporting Moran's I statistic, a two dimensional analog of the Durbin-Watson statistic,
 and  generating spatial noise to replace the dependent variable and treatment variable in turn followed by measuring how much variation in outcomes can simply be explained by spatially correlated noise.

\subsubsection*{Moran's I}
Unfortunately, whilst the dataset provided contains a `geoid2' corresponding to Census Bureau identifiers there's not enough information in the paper to map these 
into locations. Instead, I use the OpenStreetMap API \citep{OpenStreetMap} to identify reservation locations based off a string column containing reservation names. This leads 
to 156 matches and  26 unidentified locations. Using the remaining data I estimate Moran's I for our dependent variable using an MC simulation of 100,000 draws \footnote{Calculating Moran's analytically can be sensitive to irreguarly distributed polygons - i.e. US state/reservation boundaries.}
with  results displayed in Figure \ref{moran}.
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{data/output/moran-plot.png}
    \caption{Moran's I}
    \label{moran}
\end{figure}

We soundly reject the null hypothesis of no spatial autocorrelation with the realised draw (the dotted line) far to the right of 
the generated draws under the null (in pink). The $p$-value associated with the test is $0.00009$.


\subsubsection*{Spatial Noise Simulations}

To create spatial noise I generate a Gaussian process with covariance kernel:

\begin{align*}
    \Sigma(x_i, x_j) &= \exp(- || x_i - x_j ||^2) \\
    &= \Sigma_D
\end{align*}
i.e. the inverse of the squared Euclidean distance between two reservations. I sample $1000 \times 156$ draws from the corresponding 
Gaussian process:
\begin{align*}
    Y \sim MVN(\mathbf{0}_{156}, \Sigma_D)
\end{align*}


Geostatistics often uses  \textit{kriging} to interpolate between points - this is identical to using a Gaussian process and motivates 
my use of the Gaussian here. Since the covariance kernel is a function of the proximity of points any random draws will naturally exhibit spatial 
correlation and the exponential ensures correlation dies off as proximity falls. An example draw is shown in Figure \ref{noise} where lighter, larger 
points correspond to higher simulated values.


\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{data/output/noise-plot.png}
    \caption{Spatially Correlated Noise}
    \label{noise}
\end{figure}

Finally, I estimate the author's preferred IV specification using noise as the outcome variable and record the $p$-values. Since any noise 
was generated randomly with respect to the instrument/endogenous variable the only channel through which correlation can occur is through spatial 
autocorrelation \textit{despite the fact this spatial noise was generated without using any information in the covariates, dependence purely arises through 
the distance metric in the covariance kernel}. That is, if the treatment/instrument is spatially autocorrelated we can generate seemingly significant results 
even when fitting noise. Below is the quantile-quantile plot of the $p$-values - under the null hypothesis i.e. $\beta_{2SLS} = 0$ we'd expect to see $p$-values 
distributed uniformly and follow the 45 degree line.   

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.5]{data/output/qq-plot.png}
    \caption{Simulated $p$-value QQ plot}
    \label{qq}
\end{figure}

Figure \ref{qq} clearly shows the simulated $p$-values depart substantially from uniformity and it seems likely that Dippel 2014's results are at least 
in part driven by fitting spatial noise.



\FloatBarrier
\bibliographystyle{abbrvnat}
\bibliography{bib}
\end{document}